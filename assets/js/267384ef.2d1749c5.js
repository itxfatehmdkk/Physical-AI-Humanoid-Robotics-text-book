"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[5568],{2874:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter6-vla","title":"Chapter 6: Vision-Language-Action (VLA)","description":"LLM-Based Planning","source":"@site/docs/chapter6-vla.md","sourceDirName":".","slug":"/chapter6-vla","permalink":"/Physical-AI-Humanoid-Robotics-text-book/docs/chapter6-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter6-vla.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Chapter 6: Vision-Language-Action (VLA)"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: NVIDIA Isaac Platform","permalink":"/Physical-AI-Humanoid-Robotics-text-book/docs/chapter5-nvidia-isaac"},"next":{"title":"Chapter 7: Capstone: Build an Autonomous Humanoid","permalink":"/Physical-AI-Humanoid-Robotics-text-book/docs/chapter7-capstone"}}');var o=r(4848),a=r(8453);const t={sidebar_position:6,title:"Chapter 6: Vision-Language-Action (VLA)"},s="Chapter 6: Vision-Language-Action (VLA)",l={},c=[{value:"LLM-Based Planning",id:"llm-based-planning",level:2},{value:"Understanding VLA Models",id:"understanding-vla-models",level:3},{value:"Key Characteristics of LLM-Based Planning",id:"key-characteristics-of-llm-based-planning",level:3},{value:"Architecture of VLA Systems",id:"architecture-of-vla-systems",level:3},{value:"Example VLA Architecture",id:"example-vla-architecture",level:3},{value:"Training VLA Models",id:"training-vla-models",level:3},{value:"Challenges in LLM-Based Planning",id:"challenges-in-llm-based-planning",level:3},{value:"OpenAI Whisper Speech-to-Action",id:"openai-whisper-speech-to-action",level:2},{value:"Introduction to Whisper for Robotics",id:"introduction-to-whisper-for-robotics",level:3},{value:"Key Features of Whisper for Robotics",id:"key-features-of-whisper-for-robotics",level:3},{value:"Implementing Whisper in Robotics",id:"implementing-whisper-in-robotics",level:3},{value:"Advanced Whisper Integration",id:"advanced-whisper-integration",level:3},{value:"Challenges with Speech-to-Action",id:"challenges-with-speech-to-action",level:3},{value:"Converting Natural Language \u2192 ROS 2 Action Pipeline",id:"converting-natural-language--ros-2-action-pipeline",level:2},{value:"Natural Language Processing Pipeline",id:"natural-language-processing-pipeline",level:3},{value:"ROS 2 Action Servers and Clients",id:"ros-2-action-servers-and-clients",level:3},{value:"Integration with OpenAI Models",id:"integration-with-openai-models",level:3},{value:"Bipedal Locomotion &amp; Manipulation Planning",id:"bipedal-locomotion--manipulation-planning",level:2},{value:"Bipedal Locomotion Challenges",id:"bipedal-locomotion-challenges",level:3},{value:"Locomotion Planning Approaches",id:"locomotion-planning-approaches",level:3},{value:"Zero-Moment Point (ZMP) Based Control",id:"zero-moment-point-zmp-based-control",level:4},{value:"Model Predictive Control (MPC) for Walking",id:"model-predictive-control-mpc-for-walking",level:4},{value:"Manipulation Planning",id:"manipulation-planning",level:3},{value:"Coordinated Locomotion-Manipulation Planning",id:"coordinated-locomotion-manipulation-planning",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Books and Publications",id:"books-and-publications",level:3},{value:"Research Papers",id:"research-papers",level:3},{value:"Online Resources",id:"online-resources",level:3},{value:"Technical Tutorials and Tools",id:"technical-tutorials-and-tools",level:3}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-6-vision-language-action-vla",children:"Chapter 6: Vision-Language-Action (VLA)"})}),"\n",(0,o.jsx)(e.h2,{id:"llm-based-planning",children:"LLM-Based Planning"}),"\n",(0,o.jsx)(e.p,{children:"Vision-Language-Action (VLA) models represent a significant advancement in robotics, bridging the gap between high-level language understanding and low-level motor control. These models enable robots to interpret natural language commands and execute them as sequences of physical actions."}),"\n",(0,o.jsx)(e.h3,{id:"understanding-vla-models",children:"Understanding VLA Models"}),"\n",(0,o.jsx)(e.p,{children:"VLA models are large neural networks that jointly process visual inputs, language instructions, and action outputs. Unlike traditional robotics approaches where perception, planning, and control modules are separate, VLA models learn end-to-end mappings from vision and language to actions."}),"\n",(0,o.jsx)(e.h3,{id:"key-characteristics-of-llm-based-planning",children:"Key Characteristics of LLM-Based Planning"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multimodal Integration"}),": VLA models process visual and linguistic inputs simultaneously"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Zero-Shot Generalization"}),": Ability to follow novel instructions without task-specific training"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Real-World Grounding"}),": Actions are grounded in real-world visual observations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Hierarchical Reasoning"}),": Can decompose complex tasks into primitive actions"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"architecture-of-vla-systems",children:"Architecture of VLA Systems"}),"\n",(0,o.jsx)(e.p,{children:"A typical VLA system consists of several components:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Vision Encoder"}),": Processes visual input (images, video) into high-dimensional embeddings"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Language Embedder"}),": Converts natural language instructions into vector representations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Fusion Network"}),": Combines visual and language features"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Decoder"}),": Generates motor commands based on combined features"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Policy Network"}),": Maps combined features to action parameters"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"example-vla-architecture",children:"Example VLA Architecture"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torchvision.models as models\r\n\r\nclass VisionLanguageActionModel(nn.Module):\r\n    def __init__(self, action_dim, hidden_dim=512):\r\n        super().__init__()\r\n        \r\n        # Vision encoder (using ResNet as example)\r\n        self.vision_encoder = models.resnet50(pretrained=True)\r\n        self.vision_encoder.fc = nn.Identity()  # Remove final classification layer\r\n        \r\n        # Language encoder (simplified example)\r\n        self.language_encoder = nn.LSTM(\r\n            input_size=300,  # Word embedding dimension\r\n            hidden_size=hidden_dim,\r\n            batch_first=True\r\n        )\r\n        \r\n        # Fusion network\r\n        self.fusion_network = nn.Sequential(\r\n            nn.Linear(2048 + hidden_dim, hidden_dim),  # ResNet out + LSTM out\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU()\r\n        )\r\n        \r\n        # Action decoder\r\n        self.action_decoder = nn.Sequential(\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, action_dim)\r\n        )\r\n        \r\n    def forward(self, image, language):\r\n        # Process visual input\r\n        visual_features = self.vision_encoder(image)\r\n        \r\n        # Process language input\r\n        language_features, _ = self.language_encoder(language)\r\n        # Take the last output of LSTM\r\n        language_features = language_features[:, -1, :]\r\n        \r\n        # Fuse visual and language features\r\n        combined_features = torch.cat([visual_features, language_features], dim=1)\r\n        fused_features = self.fusion_network(combined_features)\r\n        \r\n        # Generate actions\r\n        actions = self.action_decoder(fused_features)\r\n        \r\n        return actions\r\n\r\n# Example usage\r\nmodel = VisionLanguageActionModel(action_dim=7)  # 7-DOF robot arm example\n"})}),"\n",(0,o.jsx)(e.h3,{id:"training-vla-models",children:"Training VLA Models"}),"\n",(0,o.jsx)(e.p,{children:"Training VLA models typically involves:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Data Collection"}),": Large datasets of demonstrations with visual, language, and action components"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Pre-training"}),": Pre-train vision and language encoders on large datasets"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Fine-tuning"}),": Fine-tune the entire system on robotics-specific data"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Curriculum Learning"}),": Gradually increase task complexity"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"challenges-in-llm-based-planning",children:"Challenges in LLM-Based Planning"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Reality Gap"}),": Models trained in simulation struggle to transfer to the real world"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Scalability"}),": Large models require significant computational resources"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Safety"}),": Ensuring actions are safe in unstructured environments"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Interpretability"}),": Understanding why models make certain decisions"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"openai-whisper-speech-to-action",children:"OpenAI Whisper Speech-to-Action"}),"\n",(0,o.jsx)(e.p,{children:"Speech interfaces provide a natural way for humans to interact with robotic systems. OpenAI's Whisper model offers state-of-the-art speech recognition capabilities that can be integrated into robotics applications."}),"\n",(0,o.jsx)(e.h3,{id:"introduction-to-whisper-for-robotics",children:"Introduction to Whisper for Robotics"}),"\n",(0,o.jsx)(e.p,{children:"Whisper is a robust automatic speech recognition (ASR) system capable of recognizing speech in various languages and acoustic conditions. When combined with robotics systems, Whisper can transform spoken commands into executable actions."}),"\n",(0,o.jsx)(e.h3,{id:"key-features-of-whisper-for-robotics",children:"Key Features of Whisper for Robotics"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multilingual Support"}),": Recognizes speech in 99 languages"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robustness"}),": Handles various accents, background noise, and acoustic conditions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Real-time Processing"}),": Can process speech with minimal latency"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Transcription Accuracy"}),": High accuracy even in challenging conditions"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"implementing-whisper-in-robotics",children:"Implementing Whisper in Robotics"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import whisper\r\nimport torch\r\nimport rospy\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\n\r\nclass SpeechToActionNode:\r\n    def __init__(self):\r\n        rospy.init_node('speech_to_action')\r\n        \r\n        # Load Whisper model (choose appropriate model size for your application)\r\n        self.model = whisper.load_model(\"base.en\")  # For English, adjust as needed\r\n        \r\n        # Publishers and subscribers\r\n        self.cmd_publisher = rospy.Publisher('cmd_vel', Twist, queue_size=10)\r\n        self.speech_subscriber = rospy.Subscriber('audio_input', String, self.speech_callback)\r\n        \r\n        # Command vocabulary for navigation\r\n        self.navigation_commands = {\r\n            'forward': (1.0, 0.0, 0.0),  # Linear x, y, z\r\n            'backward': (-1.0, 0.0, 0.0),\r\n            'left': (0.0, 0.0, 0.5),    # Angular z\r\n            'right': (0.0, 0.0, -0.5),\r\n            'stop': (0.0, 0.0, 0.0)\r\n        }\r\n        \r\n    def speech_callback(self, msg):\r\n        # Process audio file path from message\r\n        audio_path = msg.data\r\n        \r\n        # Transcribe the audio using Whisper\r\n        result = self.model.transcribe(audio_path)\r\n        text = result['text'].strip().lower()\r\n        \r\n        rospy.loginfo(f\"Speech recognized: {text}\")\r\n        \r\n        # Parse the command and execute action\r\n        self.parse_and_execute_command(text)\r\n        \r\n    def parse_and_execute_command(self, text):\r\n        # Simple keyword-based command parsing\r\n        # In practice, this would be more sophisticated\r\n        for keyword, action in self.navigation_commands.items():\r\n            if keyword in text:\r\n                cmd_msg = Twist()\r\n                cmd_msg.linear.x = action[0]\r\n                cmd_msg.linear.y = action[1]\r\n                cmd_msg.linear.z = 0.0\r\n                cmd_msg.angular.x = 0.0\r\n                cmd_msg.angular.y = 0.0\r\n                cmd_msg.angular.z = action[2]\r\n                \r\n                self.cmd_publisher.publish(cmd_msg)\r\n                rospy.loginfo(f\"Executed command: {keyword}\")\r\n                return\r\n                \r\n        rospy.logwarn(f\"Unrecognized command: {text}\")\r\n\r\ndef main():\r\n    node = SpeechToActionNode()\r\n    rospy.spin()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"advanced-whisper-integration",children:"Advanced Whisper Integration"}),"\n",(0,o.jsx)(e.p,{children:"For more sophisticated integration, Whisper can be combined with NLP models to understand complex instructions:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import whisper\r\nimport openai\r\nfrom transformers import pipeline\r\n\r\nclass AdvancedSpeechToAction:\r\n    def __init__(self, openai_api_key):\r\n        # Initialize Whisper\r\n        self.speech_model = whisper.load_model("base")\r\n        \r\n        # Initialize NLP pipeline for instruction parsing\r\n        self.instruction_parser = pipeline(\r\n            "text-classification", \r\n            model="microsoft/DialoGPT-medium"\r\n        )\r\n        \r\n        # Set up OpenAI API\r\n        openai.api_key = openai_api_key\r\n        \r\n    def process_speech_command(self, audio_path):\r\n        # Transcribe speech\r\n        result = self.speech_model.transcribe(audio_path)\r\n        text = result[\'text\'].strip()\r\n        \r\n        # Use LLM to parse complex instructions\r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-3.5-turbo",\r\n            messages=[\r\n                {"role": "system", "content": "You are a command parser for a robot. Parse the user\'s natural language command and return a structured action. Respond in JSON format with \'action_type\' and \'parameters\' fields."},\r\n                {"role": "user", "content": text}\r\n            ]\r\n        )\r\n        \r\n        # Parse the response and convert to robot action\r\n        parsed_action = self.parse_command(response.choices[0].message[\'content\'])\r\n        return parsed_action\r\n        \r\n    def parse_command(self, response_text):\r\n        # Extract structured action from LLM response\r\n        # Implementation would extract JSON from response_text\r\n        import json\r\n        try:\r\n            action_data = json.loads(response_text)\r\n            return action_data\r\n        except:\r\n            return {"action_type": "unknown", "parameters": {}}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"challenges-with-speech-to-action",children:"Challenges with Speech-to-Action"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Acoustic Conditions"}),": Performance varies with noise and environmental factors"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Real-Time Processing"}),": Need for low-latency processing in dynamic environments"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Ambient Noise"}),": Background sounds can affect recognition accuracy"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Dialects and Accents"}),": Recognition accuracy varies across different speakers"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"converting-natural-language--ros-2-action-pipeline",children:"Converting Natural Language \u2192 ROS 2 Action Pipeline"}),"\n",(0,o.jsx)(e.p,{children:"The conversion of natural language commands to executable ROS 2 actions involves multiple stages of processing and understanding."}),"\n",(0,o.jsx)(e.h3,{id:"natural-language-processing-pipeline",children:"Natural Language Processing Pipeline"}),"\n",(0,o.jsx)(e.p,{children:"The conversion process typically involves several stages:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Speech Recognition"}),": Convert speech to text (if using voice commands)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Intent Classification"}),": Determine the overall intent of the command"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Entity Extraction"}),": Identify specific objects, locations, or parameters"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Mapping"}),": Map the processed command to ROS 2 actions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Execution"}),": Execute the mapped actions in the ROS 2 system"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-action-servers-and-clients",children:"ROS 2 Action Servers and Clients"}),"\n",(0,o.jsx)(e.p,{children:"ROS 2 provides action servers and clients for handling long-running tasks:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import Pose\r\nfrom std_msgs.msg import String\r\nimport json\r\n\r\nclass NaturalLanguageActionServer(Node):\r\n    def __init__(self):\r\n        super().__init__('natural_language_action_server')\r\n        \r\n        # Create action server for navigation tasks\r\n        self._action_server = ActionServer(\r\n            self,\r\n            NavigateWithCommand,\r\n            'navigate_with_command',\r\n            execute_callback=self.execute_callback,\r\n            goal_callback=self.goal_callback,\r\n            cancel_callback=self.cancel_callback)\r\n        \r\n        # Initialize NLP components\r\n        self.nlp_processor = NLPProcessor()  # Custom NLP class\r\n        \r\n        # Publishers for navigation\r\n        self.nav_publisher = self.create_publisher(Pose, 'goal_pose', 10)\r\n\r\n    def goal_callback(self, goal_request):\r\n        \"\"\"Accept or reject goal requests.\"\"\"\r\n        self.get_logger().info('Received goal request')\r\n        return GoalResponse.ACCEPT\r\n\r\n    def cancel_callback(self, goal_handle):\r\n        \"\"\"Accept or reject cancel requests.\"\"\"\r\n        self.get_logger().info('Received cancel request')\r\n        return CancelResponse.ACCEPT\r\n    \r\n    def execute_callback(self, goal_handle):\r\n        \"\"\"Execute the goal.\"\"\"\r\n        self.get_logger().info('Executing goal...')\r\n        \r\n        # Process the natural language command\r\n        command_text = goal_handle.request.command\r\n        parsed_command = self.nlp_processor.parse_command(command_text)\r\n        \r\n        # Execute the parsed command\r\n        result = self.execute_parsed_command(parsed_command)\r\n        \r\n        # Return result\r\n        goal_handle.succeed()\r\n        result = NavigateWithCommand.Result()\r\n        result.success = True\r\n        result.message = f\"Executed command: {command_text}\"\r\n        return result\r\n\r\n    def execute_parsed_command(self, parsed_command):\r\n        \"\"\"Execute a parsed command.\"\"\"\r\n        command_type = parsed_command.get('type')\r\n        \r\n        if command_type == 'navigation':\r\n            # Extract goal position\r\n            x = parsed_command.get('x', 0.0)\r\n            y = parsed_command.get('y', 0.0)\r\n            \r\n            # Create and publish goal pose\r\n            goal_pose = Pose()\r\n            goal_pose.position.x = x\r\n            goal_pose.position.y = y\r\n            goal_pose.position.z = 0.0\r\n            self.nav_publisher.publish(goal_pose)\r\n            \r\n        elif command_type == 'manipulation':\r\n            # Handle manipulation commands\r\n            object_name = parsed_command.get('object')\r\n            action = parsed_command.get('action')\r\n            # Implementation for manipulation would go here\r\n            \r\n        return True\r\n\r\nclass NLPProcessor:\r\n    \"\"\"Process natural language commands.\"\"\"\r\n    def __init__(self):\r\n        # Initialize NLP models\r\n        pass\r\n    \r\n    def parse_command(self, command_text):\r\n        \"\"\"Parse a natural language command.\"\"\"\r\n        # This is a simplified example\r\n        # In practice, you would use more sophisticated NLP techniques\r\n        \r\n        command_text_lower = command_text.lower()\r\n        \r\n        if 'go to' in command_text_lower or 'navigate to' in command_text_lower:\r\n            # Extract location (this is a simplified extraction)\r\n            import re\r\n            # Simple regex to extract coordinates\r\n            match = re.search(r'(\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)', command_text)\r\n            if match:\r\n                x, y = float(match.group(1)), float(match.group(2))\r\n                return {\r\n                    'type': 'navigation',\r\n                    'x': x,\r\n                    'y': y\r\n                }\r\n            else:\r\n                # Handle named locations\r\n                if 'kitchen' in command_text_lower:\r\n                    return {\r\n                        'type': 'navigation',\r\n                        'x': 5.0,\r\n                        'y': 3.0\r\n                    }\r\n                elif 'living room' in command_text_lower:\r\n                    return {\r\n                        'type': 'navigation',\r\n                        'x': -2.0,\r\n                        'y': 1.0\r\n                    }\r\n        \r\n        elif 'pick up' in command_text_lower or 'grasp' in command_text_lower:\r\n            # Extract object to manipulate\r\n            import re\r\n            # Extract object name\r\n            match = re.search(r'(?:pick up|grasp|get)\\s+(.+)', command_text)\r\n            if match:\r\n                object_name = match.group(1).strip()\r\n                return {\r\n                    'type': 'manipulation',\r\n                    'action': 'pick_up',\r\n                    'object': object_name\r\n                }\r\n        \r\n        # Default case - unrecognized command\r\n        return {\r\n            'type': 'unknown'\r\n        }\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    action_server = NaturalLanguageActionServer()\r\n    rclpy.spin(action_server)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"integration-with-openai-models",children:"Integration with OpenAI Models"}),"\n",(0,o.jsx)(e.p,{children:"For more sophisticated natural language understanding, we can use OpenAI models:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import openai\r\nimport json\r\n\r\nclass OpenAICommandProcessor:\r\n    def __init__(self, api_key):\r\n        openai.api_key = api_key\r\n        \r\n    def parse_command(self, command_text):\r\n        """Use OpenAI to parse natural language commands."""\r\n        prompt = f"""\r\n        Parse the following natural language command for a robot and return the structured output in JSON format:\r\n        \r\n        Command: "{command_text}"\r\n        \r\n        Return a JSON object with the following structure:\r\n        {{\r\n          "action_type": "navigation | manipulation | interaction | etc.",\r\n          "parameters": {{\r\n            "x": float, // If navigation\r\n            "y": float, // If navigation\r\n            "object": "string", // If manipulation\r\n            "action": "string"  // If manipulation\r\n          }},\r\n          "confidence": float // Between 0 and 1\r\n        }}\r\n        \r\n        Be precise with the JSON format and only return the JSON object with no additional text.\r\n        """\r\n        \r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-3.5-turbo",\r\n            messages=[{"role": "user", "content": prompt}],\r\n            temperature=0.1  # Low temperature for more deterministic output\r\n        )\r\n        \r\n        try:\r\n            # Extract JSON from response\r\n            content = response.choices[0].message[\'content\'].strip()\r\n            \r\n            # Sometimes the API might return the JSON wrapped in markdown code blocks\r\n            if content.startswith(\'```\'):\r\n                content = content[content.find(\'{\'):content.rfind(\'}\')+1]\r\n            \r\n            parsed_command = json.loads(content)\r\n            return parsed_command\r\n        except Exception as e:\r\n            print(f"Error parsing command: {e}")\r\n            return {\r\n                "action_type": "unknown",\r\n                "parameters": {},\r\n                "confidence": 0.0\r\n            }\n'})}),"\n",(0,o.jsx)(e.h2,{id:"bipedal-locomotion--manipulation-planning",children:"Bipedal Locomotion & Manipulation Planning"}),"\n",(0,o.jsx)(e.p,{children:"Combining locomotion and manipulation planning in humanoid robots is one of the most challenging problems in robotics, requiring coordination between the robot's ability to move around and interact with objects in its environment."}),"\n",(0,o.jsx)(e.h3,{id:"bipedal-locomotion-challenges",children:"Bipedal Locomotion Challenges"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots face several unique challenges for locomotion:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Balance"}),": Maintaining stability on two legs"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Dynamic Walking"}),": Managing the physics of walking"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Terrain Adaptation"}),": Adapting to different surfaces and obstacles"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Energy Efficiency"}),": Minimizing power consumption"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"locomotion-planning-approaches",children:"Locomotion Planning Approaches"}),"\n",(0,o.jsx)(e.h4,{id:"zero-moment-point-zmp-based-control",children:"Zero-Moment Point (ZMP) Based Control"}),"\n",(0,o.jsx)(e.p,{children:"ZMP control ensures the robot maintains balance by keeping the center of pressure within the support polygon:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\n\r\nclass ZMPBasedWalker:\r\n    def __init__(self, robot_mass, gravity=9.81):\r\n        self.mass = robot_mass\r\n        self.gravity = gravity\r\n        self.g = gravity\r\n        self.omega = np.sqrt(self.g / self.mass)  # Simplified omega for ZMP\r\n        \r\n    def compute_zmp_reference(self, com_trajectory, com_vel_trajectory, com_acc_trajectory):\r\n        """\r\n        Compute ZMP reference from center of mass trajectory\r\n        This is a simplified version - real implementations are more complex\r\n        """\r\n        zmp_ref_x = com_trajectory[:, 0] - (com_acc_trajectory[:, 0] / self.omega**2)\r\n        zmp_ref_y = com_trajectory[:, 1] - (com_acc_trajectory[:, 1] / self.omega**2)\r\n        \r\n        return np.column_stack((zmp_ref_x, zmp_ref_y))\r\n\r\n    def generate_foot_steps(self, zmp_reference, step_length=0.3, step_width=0.2):\r\n        """\r\n        Generate footstep plan based on ZMP reference\r\n        """\r\n        footsteps = []\r\n        \r\n        # Simplified footstep generation\r\n        # In practice, this would implement more sophisticated planning\r\n        for i, zmp_point in enumerate(zmp_reference):\r\n            if i % 2 == 0:  # Left foot\r\n                foot_pos = [zmp_point[0], step_width/2.0, 0.0]\r\n            else:  # Right foot\r\n                foot_pos = [zmp_point[0], -step_width/2.0, 0.0]\r\n                \r\n            footsteps.append(foot_pos)\r\n            \r\n        return footsteps\n'})}),"\n",(0,o.jsx)(e.h4,{id:"model-predictive-control-mpc-for-walking",children:"Model Predictive Control (MPC) for Walking"}),"\n",(0,o.jsx)(e.p,{children:"MPC approaches optimize walking patterns over a prediction horizon:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class ModelPredictiveWalker:\r\n    def __init__(self, prediction_horizon=20, dt=0.1):\r\n        self.horizon = prediction_horizon\r\n        self.dt = dt\r\n        self.A = np.array([[1, self.dt, self.dt**2 / 2],\r\n                           [0, 1, self.dt],\r\n                           [0, 0, 1]])\r\n        # Simplified dynamics matrix for CoM motion\r\n        \r\n    def compute_control_sequence(self, current_state, reference_trajectory):\r\n        """\r\n        Compute control sequence using simplified MPC approach\r\n        """\r\n        # This is a simplified implementation\r\n        # Real MPC would solve an optimization problem\r\n        control_sequence = np.zeros((self.horizon, 2))  # 2D control (x, y)\r\n        \r\n        for i in range(self.horizon):\r\n            # Simplified tracking control\r\n            error = reference_trajectory[i] - current_state[:2]\r\n            control_sequence[i] = 2 * error  # Simplified control law\r\n            \r\n        return control_sequence\n'})}),"\n",(0,o.jsx)(e.h3,{id:"manipulation-planning",children:"Manipulation Planning"}),"\n",(0,o.jsx)(e.p,{children:"Manipulation planning involves planning robot arm motions to interact with objects:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import numpy as np\r\n\r\nclass ManipulationPlanner:\r\n    def __init__(self, robot):\r\n        self.robot = robot\r\n        self.workspace_bounds = {\r\n            'x': (-0.5, 0.5),\r\n            'y': (-0.3, 0.3),\r\n            'z': (0.1, 0.8)\r\n        }\r\n        \r\n    def plan_reach_motion(self, target_pose):\r\n        \"\"\"\r\n        Plan arm motion to reach a target pose\r\n        \"\"\"\r\n        # Simplified motion planning\r\n        # In practice, this would use more sophisticated planners like RRT*\r\n        current_pose = self.robot.get_end_effector_pose()\r\n        \r\n        # Linear interpolation in Cartesian space\r\n        num_steps = 20\r\n        trajectory = []\r\n        \r\n        for i in range(num_steps + 1):\r\n            t = i / num_steps\r\n            interpolated_pose = {\r\n                'x': current_pose['x'] + t * (target_pose['x'] - current_pose['x']),\r\n                'y': current_pose['y'] + t * (target_pose['y'] - current_pose['y']),\r\n                'z': current_pose['z'] + t * (target_pose['z'] - current_pose['z'])\r\n            }\r\n            trajectory.append(interpolated_pose)\r\n            \r\n        return trajectory\r\n    \r\n    def plan_grasp(self, object_pose):\r\n        \"\"\"\r\n        Plan approach, grasp, and lift motion for an object\r\n        \"\"\"\r\n        # Approach position (slightly above the object)\r\n        approach_pose = object_pose.copy()\r\n        approach_pose['z'] += 0.15  # 15cm above object\r\n        \r\n        # Grasp position (at object height)\r\n        grasp_pose = object_pose.copy()\r\n        \r\n        # Lift position (above object)\r\n        lift_pose = object_pose.copy()\r\n        lift_pose['z'] += 0.2  # Lift 20cm\r\n        \r\n        # Generate the sequence of motions\r\n        approach_traj = self.plan_reach_motion(approach_pose)\r\n        grasp_traj = self.plan_reach_motion(grasp_pose)\r\n        lift_traj = self.plan_reach_motion(lift_pose)\r\n        \r\n        return approach_traj + grasp_traj + lift_traj\n"})}),"\n",(0,o.jsx)(e.h3,{id:"coordinated-locomotion-manipulation-planning",children:"Coordinated Locomotion-Manipulation Planning"}),"\n",(0,o.jsx)(e.p,{children:"For humanoid robots, coordinating walking and manipulation is essential:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class CoordinatedPlanner:\r\n    def __init__(self, locomotion_planner, manipulation_planner):\r\n        self.loco_planner = locomotion_planner\r\n        self.mani_planner = manipulation_planner\r\n        \r\n    def plan_task(self, goal_location, object_to_manipulate):\r\n        """\r\n        Plan coordinated task involving both locomotion and manipulation\r\n        """\r\n        # 1. Plan path to approach the object\r\n        navigation_plan = self.loco_planner.plan_to_object(\r\n            object_to_manipulate[\'location\']\r\n        )\r\n        \r\n        # 2. Plan manipulation at the location\r\n        manipulation_plan = self.mani_planner.plan_grasp(object_to_manipulate[\'pose\'])\r\n        \r\n        # 3. Coordinate the plans to ensure balance during manipulation\r\n        coordinated_plan = self.coordinate_loco_mani(navigation_plan, manipulation_plan)\r\n        \r\n        return coordinated_plan\r\n        \r\n    def coordinate_loco_mani(self, navigation_plan, manipulation_plan):\r\n        """\r\n        Coordinate locomotion and manipulation to maintain stability\r\n        """\r\n        # Simplified coordination strategy:\r\n        # - Stop locomotion during manipulation\r\n        # - Use upper body for manipulation while maintaining base stability\r\n        \r\n        # In practice, this would implement more sophisticated coordination\r\n        # such as dynamic balancing or anticipatory postural adjustments\r\n        \r\n        return {\r\n            \'navigation_plan\': navigation_plan,\r\n            \'manipulation_plan\': manipulation_plan,\r\n            \'timing\': self.calculate_timing(navigation_plan, manipulation_plan)\r\n        }\r\n        \r\n    def calculate_timing(self, nav_plan, mani_plan):\r\n        """\r\n        Calculate timing to coordinate locomotion and manipulation\r\n        """\r\n        # Simplified timing calculation\r\n        return {\r\n            \'start_manipulation_at_step\': len(nav_plan) // 2,\r\n            \'manipulation_duration\': len(mani_plan) * 0.1  # 0.1s per step\r\n        }\n'})}),"\n",(0,o.jsx)(e.p,{children:"Vision-Language-Action (VLA) models represent the cutting edge of robotics research, enabling robots to understand and respond to natural language commands through coordinated physical actions. By combining the power of large language models with perception and control systems, VLA systems are bringing us closer to truly general-purpose robots that can interact naturally with humans in complex environments."}),"\n",(0,o.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(e.p,{children:"This chapter has explored the critical VLA systems that bridge language understanding with physical action in robotics. From LLM-based planning to speech-to-action pipelines and coordinated locomotion-manipulation planning, these technologies are essential for creating robots that can effectively interact with the physical world through natural language commands. The next chapter will integrate all these concepts in a capstone project to build an autonomous humanoid system."}),"\n",(0,o.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,o.jsx)(e.p,{children:"For readers interested in exploring these concepts at a deeper level:"}),"\n",(0,o.jsx)(e.h3,{id:"books-and-publications",children:"Books and Publications"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://www.tandf.net/books/9781032058528",children:'"Language and Robots" by Antonio Lieto'})," - Intersections between linguistics and robotics"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://www.morganclaypool.com/doi/abs/10.2200/S00629ED1V01Y201501AIM029",children:'"Robot Learning from Human Teachers" by Sonia Chernova and Andrea Thomaz'})," - Human-in-the-loop learning approaches"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://www.apress.com/gp/book/9781484243534",children:'"Deep Learning for Natural Language Processing" by Palash Goyal'})," - Foundation for language understanding systems"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://link.springer.com/book/10.1007/978-1-4020-5587-4",children:'"Handbook of Spatial Logics" by Marco Aiello'})," - Spatial reasoning for robotics"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2201.07207",children:'"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents" by Huang et al. (2022)'})," - Using LLMs for robotic planning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2207.05608",children:'"Inner Monologue: Embodied Reasoning through Planning in Language Models" by Brohan et al. (2022)'})," - Language-based reasoning for embodied tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2303.03378",children:'"PaLM-E: An Embodied Multimodal Language Model" by Driess et al. (2023)'})," - Large-scale vision-language-action model"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2205.14818",children:'"Grounded Decoding for Language-Guided Multi-Object Rearrangement" by Chen et al. (2022)'})," - Language-guided manipulation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2203.06844",children:'"Language-Conditioned Imitation Learning for Robot Manipulation Tasks" by Shridhar et al. (2022)'})," - Learning manipulation from language demonstrations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2105.08444",children:'"Embodied Visual Active Learning for Semantic Segmentation" by Zhang et al. (2021)'})," - Active learning for perception"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2212.06817",children:'"RT-1: Robotics Transformer for Real-World Control at Scale" by Brohan et al. (2022)'})," - Scalable robotics transformer model"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://openvla.github.io/",children:'"OpenVLA: An Open-Source Vision-Language-Action Model" by Bahl et al. (2024)'})," - Open-source VLA model"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"online-resources",children:"Online Resources"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://platform.openai.com/docs/api-reference",children:"OpenAI API Documentation"})," - Interface for language models in robotics"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://huggingface.co/docs/transformers/index",children:"Hugging Face Transformers"})," - Pre-trained models for NLP tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://sites.google.com/view/robotics-transformers",children:"Robot Learning with Language Models"})," - Research hub for VLA systems"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://aihabitat.org/challenge/",children:"Embodied AI Challenge"})," - Annual challenge focused on embodied intelligence"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://roboturk.stanford.edu/",children:"RoboTurk: Human Activities in Robot Environments"})," - Dataset for language-guided manipulation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://askforalfred.com/",children:"ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks"})," - Task-oriented dataset with language instructions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://say-can.github.io/",children:"SayCan: Do As I Can, Not As I Say"})," - Language-guided robot execution"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"technical-tutorials-and-tools",children:"Technical Tutorials and Tools"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper Documentation"})," - Speech recognition for robotics applications"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://github.com/facebookresearch/SEARLE",children:"Speech-to-Action Pipeline Implementation"})," - Example implementation of speech-driven robot control"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://huggingface.co/docs/transformers/model_doc/vision_encoder_decoder",children:"Vision-Language-Action Model Training Guide"})," - Training VLA models"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://github.com/ros-industrial/ros2_nlp",children:"ROS 2 Natural Language Processing Integration"})," - NLP tools for ROS 2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://github.com/google-research/language_to_reward",children:"Language-Guided Reinforcement Learning Tutorials"})," - LfR (Language to Reward) implementations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://github.com/facebookresearch/habitat-lab",children:"Embodied-AI Simulation Environments"})," - Habitat for embodied AI research"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://www.promptingguide.ai/",children:"Prompt Engineering for Robotics"})," - Techniques for designing effective prompts for robotic systems"]}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>t,x:()=>s});var i=r(6540);const o={},a=i.createContext(o);function t(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:t(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);