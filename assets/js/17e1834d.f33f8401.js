"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[2926],{239:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"chapter7-capstone","title":"Chapter 7: Capstone: Build an Autonomous Humanoid","description":"Voice \u2192 Plan \u2192 Movement \u2192 Object Identification \u2192 Manipulation","source":"@site/docs/chapter7-capstone.md","sourceDirName":".","slug":"/chapter7-capstone","permalink":"/Physical-AI-Humanoid-Robotics-text-book/docs/chapter7-capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter7-capstone.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"Chapter 7: Capstone: Build an Autonomous Humanoid"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6: Vision-Language-Action (VLA)","permalink":"/Physical-AI-Humanoid-Robotics-text-book/docs/chapter6-vla"}}');var i=t(4848),a=t(8453);const s={sidebar_position:7,title:"Chapter 7: Capstone: Build an Autonomous Humanoid"},r="Chapter 7: Capstone: Build an Autonomous Humanoid",c={},l=[{value:"Voice \u2192 Plan \u2192 Movement \u2192 Object Identification \u2192 Manipulation",id:"voice--plan--movement--object-identification--manipulation",level:2},{value:"System Architecture Overview",id:"system-architecture-overview",level:3},{value:"High-Level System Pipeline",id:"high-level-system-pipeline",level:3},{value:"Voice Command Processing",id:"voice-command-processing",level:3},{value:"Task Planning System",id:"task-planning-system",level:3},{value:"Full Pipeline with ROS 2 + Isaac + LLM",id:"full-pipeline-with-ros-2--isaac--llm",level:2},{value:"ROS 2 Node Integration",id:"ros-2-node-integration",level:3},{value:"Isaac Platform Integration",id:"isaac-platform-integration",level:3},{value:"Object Identification with Computer Vision",id:"object-identification-with-computer-vision",level:3},{value:"Deployment Blueprint",id:"deployment-blueprint",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Computing Platform",id:"computing-platform",level:4},{value:"Sensors",id:"sensors",level:4},{value:"Actuators",id:"actuators",level:4},{value:"Software Stack",id:"software-stack",level:3},{value:"Deployment Configuration",id:"deployment-configuration",level:3},{value:"Launch File Configuration",id:"launch-file-configuration",level:3},{value:"Deployment Steps",id:"deployment-steps",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-7-capstone-build-an-autonomous-humanoid",children:"Chapter 7: Capstone: Build an Autonomous Humanoid"})}),"\n",(0,i.jsx)(e.h2,{id:"voice--plan--movement--object-identification--manipulation",children:"Voice \u2192 Plan \u2192 Movement \u2192 Object Identification \u2192 Manipulation"}),"\n",(0,i.jsx)(e.p,{children:"In this capstone chapter, we integrate all the concepts from previous chapters to create an autonomous humanoid robot capable of receiving voice commands, planning actions, executing movement, identifying objects, and performing manipulation tasks. This represents the convergence of the physical and digital AI worlds."}),"\n",(0,i.jsx)(e.h3,{id:"system-architecture-overview",children:"System Architecture Overview"}),"\n",(0,i.jsx)(e.p,{children:"The autonomous humanoid system combines several key technologies:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Natural language processing for voice command interpretation"}),"\n",(0,i.jsx)(e.li,{children:"Path planning and navigation for movement"}),"\n",(0,i.jsx)(e.li,{children:"Computer vision for object identification"}),"\n",(0,i.jsx)(e.li,{children:"Manipulation planning and control"}),"\n",(0,i.jsx)(e.li,{children:"Locomotion control for bipedal walking"}),"\n",(0,i.jsx)(e.li,{children:"Integration with NVIDIA Isaac platform for acceleration"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"high-level-system-pipeline",children:"High-Level System Pipeline"}),"\n",(0,i.jsx)(e.p,{children:"The complete pipeline from voice to action follows these stages:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Voice Input"}),": Receive spoken command through audio input"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Natural Language Processing"}),": Convert voice to text and interpret intent"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task Planning"}),": Decompose high-level command into executable subtasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Navigation Planning"}),": Plan path to relevant locations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Object Detection & Identification"}),": Identify objects of interest"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Manipulation Planning"}),": Plan grasp and manipulation motions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Execution"}),": Execute coordinated locomotion and manipulation"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"voice-command-processing",children:"Voice Command Processing"}),"\n",(0,i.jsx)(e.p,{children:"The robot's voice processing system uses both Whisper for speech recognition and a language model for command interpretation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import whisper\nimport openai\nimport json\nfrom typing import Dict, Any, List\n\nclass VoiceCommandProcessor:\n    def __init__(self, openai_api_key: str):\n        self.speech_model = whisper.load_model("base.en")\n        openai.api_key = openai_api_key\n        self.command_history = []\n        \n    def process_voice_command(self, audio_path: str) -> Dict[str, Any]:\n        """\n        Process voice command and return structured action plan\n        """\n        # 1. Transcribe speech to text\n        transcription = self.speech_model.transcribe(audio_path)\n        text_command = transcription[\'text\'].strip()\n        \n        print(f"Interpreted command: {text_command}")\n        \n        # 2. Use OpenAI to parse command and create action plan\n        prompt = f"""\n        You are a command parser for an autonomous humanoid robot. \n        Convert the following natural language command into a structured action plan.\n        \n        Command: "{text_command}"\n        \n        Return a JSON object with this structure:\n        {{\n          "command": "{text_command}",\n          "primary_action": "navigation | manipulation | interaction",\n          "target": "location | object | person",\n          "parameters": {{\n            "destination": [x, y, z],\n            "object_name": "object identifier",\n            "action": "grasp | move | place | etc."\n          }},\n          "subtasks": [\n            {{\n              "type": "navigation | perception | manipulation",\n              "description": "specific task"\n            }}\n          ]\n        }}\n        \n        Only return the JSON object with no additional text.\n        """\n        \n        response = openai.ChatCompletion.create(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": prompt}],\n            temperature=0.1\n        )\n        \n        try:\n            content = response.choices[0].message[\'content\'].strip()\n            \n            # Extract JSON from response\n            if content.startswith(\'```\'):\n                start_idx = content.find(\'{\')\n                end_idx = content.rfind(\'}\') + 1\n                content = content[start_idx:end_idx]\n                \n            parsed_command = json.loads(content)\n            self.command_history.append(parsed_command)\n            return parsed_command\n            \n        except Exception as e:\n            print(f"Error parsing command: {e}")\n            return {\n                "command": text_command,\n                "primary_action": "unknown",\n                "target": "unknown",\n                "parameters": {},\n                "subtasks": []\n            }\n'})}),"\n",(0,i.jsx)(e.h3,{id:"task-planning-system",children:"Task Planning System"}),"\n",(0,i.jsx)(e.p,{children:"The task planning system decomposes high-level commands into executable actions:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from enum import Enum\nfrom typing import List, Tuple\n\nclass TaskType(Enum):\n    NAVIGATION = \"navigation\"\n    PERCEPTION = \"perception\"\n    MANIPULATION = \"manipulation\"\n    LOCOMOTION = \"locomotion\"\n\nclass TaskPlan:\n    def __init__(self):\n        self.tasks = []\n        self.current_task_index = 0\n        \n    def add_task(self, task_type: TaskType, description: str, params: dict):\n        self.tasks.append({\n            'type': task_type,\n            'description': description,\n            'parameters': params,\n            'completed': False\n        })\n        \n    def get_next_task(self):\n        if self.current_task_index < len(self.tasks):\n            task = self.tasks[self.current_task_index]\n            self.current_task_index += 1\n            return task\n        return None\n        \n    def mark_task_completed(self, task_index: int):\n        if 0 <= task_index < len(self.tasks):\n            self.tasks[task_index]['completed'] = True\n\nclass TaskPlanner:\n    def __init__(self):\n        self.task_plan = TaskPlan()\n        \n    def plan_task(self, command: Dict[str, Any]) -> TaskPlan:\n        \"\"\"\n        Create a task plan from a high-level command\n        \"\"\"\n        self.task_plan = TaskPlan()\n        \n        primary_action = command.get('primary_action', 'unknown')\n        \n        if primary_action == 'navigation':\n            # Add navigation tasks\n            destination = command['parameters'].get('destination', [0, 0, 0])\n            self.task_plan.add_task(\n                TaskType.NAVIGATION,\n                f\"Navigate to location [{destination[0]:.2f}, {destination[1]:.2f}, {destination[2]:.2f}]\",\n                {'destination': destination}\n            )\n            \n        elif primary_action == 'manipulation':\n            # Add perception and manipulation tasks\n            object_name = command['parameters'].get('object_name', 'unknown')\n            action = command['parameters'].get('action', 'grasp')\n            \n            self.task_plan.add_task(\n                TaskType.PERCEPTION,\n                f\"Detect and locate {object_name}\",\n                {'object_type': object_name}\n            )\n            \n            self.task_plan.add_task(\n                TaskType.MANIPULATION,\n                f\"{action.capitalize()} {object_name}\",\n                {'object_name': object_name, 'action': action}\n            )\n            \n        elif primary_action == 'interaction':\n            # Handle complex interactions\n            for subtask in command.get('subtasks', []):\n                task_type = TaskType[subtask['type'].upper()]\n                self.task_plan.add_task(\n                    task_type,\n                    subtask['description'],\n                    {}\n                )\n                \n        return self.task_plan\n"})}),"\n",(0,i.jsx)(e.h2,{id:"full-pipeline-with-ros-2--isaac--llm",children:"Full Pipeline with ROS 2 + Isaac + LLM"}),"\n",(0,i.jsx)(e.h3,{id:"ros-2-node-integration",children:"ROS 2 Node Integration"}),"\n",(0,i.jsx)(e.p,{children:"The main autonomous humanoid node integrates all components:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport cv2\nimport numpy as np\nfrom cv_bridge import CvBridge\n\nclass AutonomousHumanoidNode(Node):\n    def __init__(self):\n        super().__init__(\'autonomous_humanoid\')\n        \n        # Initialize components\n        self.voice_processor = VoiceCommandProcessor(openai_api_key="YOUR_API_KEY")\n        self.task_planner = TaskPlanner()\n        self.bridge = CvBridge()\n        \n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, \'goal_pose\', 10)\n        \n        # Subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, \'audio_input\', self.audio_callback, 10)\n        self.camera_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.camera_callback, 10)\n        \n        # Timers\n        self.control_timer = self.create_timer(0.1, self.control_loop)\n        \n        # State variables\n        self.current_task_plan = None\n        self.robot_pose = np.array([0.0, 0.0, 0.0])  # x, y, theta\n        self.is_executing_task = False\n        \n    def audio_callback(self, msg):\n        """\n        Handle incoming audio data\n        """\n        if self.is_executing_task:\n            self.get_logger().info("Robot busy with current task, ignoring voice command")\n            return\n            \n        # Save audio data temporarily\n        audio_path = "/tmp/received_audio.wav"  # In real implementation, handle audio properly\n        \n        # Process voice command\n        command = self.voice_processor.process_voice_command(audio_path)\n        \n        # Create task plan\n        self.current_task_plan = self.task_planner.plan_task(command)\n        self.is_executing_task = True\n        \n        self.get_logger().info(f"Created task plan with {len(self.current_task_plan.tasks)} tasks")\n        \n    def camera_callback(self, msg):\n        """\n        Process camera data for object detection\n        """\n        cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n        \n        # Object detection would happen here\n        # For now, we\'ll just log the image shape\n        self.get_logger().info(f"Received image: {cv_image.shape}")\n        \n    def control_loop(self):\n        """\n        Main control loop for executing tasks\n        """\n        if not self.is_executing_task or not self.current_task_plan:\n            return\n            \n        # Get next task to execute\n        current_task = self.current_task_plan.get_next_task()\n        \n        if current_task:\n            self.get_logger().info(f"Executing task: {current_task[\'description\']}")\n            \n            # Execute the task based on its type\n            if current_task[\'type\'] == TaskType.NAVIGATION:\n                self.execute_navigation_task(current_task)\n            elif current_task[\'type\'] == TaskType.PERCEPTION:\n                self.execute_perception_task(current_task)\n            elif current_task[\'type\'] == TaskType.MANIPULATION:\n                self.execute_manipulation_task(current_task)\n            elif current_task[\'type\'] == TaskType.LOCOMOTION:\n                self.execute_locomotion_task(current_task)\n        else:\n            # All tasks completed\n            self.get_logger().info("All tasks completed")\n            self.is_executing_task = False\n            self.current_task_plan = None\n            \n    def execute_navigation_task(self, task):\n        """\n        Execute navigation task\n        """\n        destination = task[\'parameters\'][\'destination\']\n        \n        # Create goal message\n        goal_msg = PoseStamped()\n        goal_msg.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.header.frame_id = \'map\'\n        goal_msg.pose.position.x = destination[0]\n        goal_msg.pose.position.y = destination[1]\n        goal_msg.pose.position.z = destination[2]  # height\n        goal_msg.pose.orientation.w = 1.0  # No rotation\n        \n        self.goal_pub.publish(goal_msg)\n        \n        # For now, just mark as completed after publishing\n        self.current_task_plan.mark_task_completed(self.current_task_plan.current_task_index - 1)\n        \n    def execute_perception_task(self, task):\n        """\n        Execute perception task (object detection)\n        """\n        object_type = task[\'parameters\'][\'object_type\']\n        \n        # In real implementation, this would:\n        # 1. Use camera data to detect the specified object type\n        # 2. Determine the object\'s location relative to the robot\n        # 3. Update robot\'s knowledge about the environment\n        \n        self.get_logger().info(f"Searching for {object_type}")\n        \n        # Mark as completed\n        self.current_task_plan.mark_task_completed(self.current_task_plan.current_task_index - 1)\n        \n    def execute_manipulation_task(self, task):\n        """\n        Execute manipulation task\n        """\n        object_name = task[\'parameters\'][\'object_name\']\n        action = task[\'parameters\'][\'action\']\n        \n        # In real implementation, this would:\n        # 1. Plan manipulation trajectory to the object\n        # 2. Execute grasping or other manipulation action\n        # 3. Verify success of the manipulation\n        \n        self.get_logger().info(f"Attempting to {action} {object_name}")\n        \n        # Mark as completed\n        self.current_task_plan.mark_task_completed(self.current_task_plan.current_task_index - 1)\n        \n    def execute_locomotion_task(self, task):\n        """\n        Execute locomotion task (for bipedal walking)\n        """\n        # In real implementation, this would:\n        # 1. Generate walking pattern using ZMP or similar control\n        # 2. Execute coordinated joint motions for walking\n        # 3. Maintain balance during locomotion\n        \n        self.get_logger().info("Executing locomotion task")\n        \n        # Mark as completed\n        self.current_task_plan.mark_task_completed(self.current_task_plan.current_task_index - 1)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    human_node = AutonomousHumanoidNode()\n    \n    try:\n        rclpy.spin(human_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        human_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"isaac-platform-integration",children:"Isaac Platform Integration"}),"\n",(0,i.jsx)(e.p,{children:"Integrating with NVIDIA Isaac for perception and acceleration:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import Point\nfrom isaac_ros_visual_slam_msgs.msg import GraphState, TrackState\nfrom std_msgs.msg import Float64\nimport torch\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass IsaacHumanoidNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_humanoid_node\')\n        \n        self.bridge = CvBridge()\n        \n        # Isaac-specific publishers/subscribers\n        self.odometry_sub = self.create_subscription(\n            GraphState, \'visual_slam/fixed_landmarks\', self.odometry_callback, 10)\n        self.image_sub = self.create_subscription(\n            Image, \'camera/color/image_raw\', self.image_callback, 10)\n        self.depth_sub = self.create_subscription(\n            Image, \'camera/depth/image_rect_raw\', self.depth_callback, 10)\n        \n        # Publishers for Isaac-accelerated perception\n        self.object_position_pub = self.create_publisher(\n            Point, \'detected_object_position\', 10)\n        \n        # Initialize Isaac-accelerated perception models\n        self.initialize_isaac_perception()\n        \n    def initialize_isaac_perception(self):\n        """\n        Initialize Isaac-accelerated perception models\n        """\n        # Load Isaac object detection model (pseudo-code)\n        # In real implementation, this would load Isaac\'s perception models\n        pass\n        \n    def image_callback(self, msg):\n        """\n        Process camera image using Isaac-accelerated perception\n        """\n        cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n        \n        # Use Isaac-accelerated object detection\n        detected_objects = self.isaac_object_detection(cv_image)\n        \n        if detected_objects:\n            # Publish object positions\n            for obj in detected_objects:\n                pos_msg = Point()\n                pos_msg.x = obj[\'x\']\n                pos_msg.y = obj[\'y\']\n                pos_msg.z = obj[\'z\']\n                self.object_position_pub.publish(pos_msg)\n        \n    def isaac_object_detection(self, image):\n        """\n        Perform object detection using Isaac-accelerated models\n        """\n        # This would use actual Isaac ROS perception packages\n        # For example, using Isaac ROS Detection or similar\n        # Return list of detected objects with positions\n        \n        # Pseudo-code for Isaac integration\n        """\n        objects = []\n        \n        # Run Isaac perception pipeline\n        detections = self.isaac_detector.detect(image)\n        \n        for detection in detections:\n            # Convert to world coordinates using depth and camera parameters\n            world_pos = self.convert_to_world_coords(\n                detection.x, detection.y, detection.depth)\n            objects.append({\n                \'name\': detection.class_name,\n                \'x\': world_pos.x,\n                \'y\': world_pos.y,\n                \'z\': world_pos.z\n            })\n        \n        return objects\n        """\n        \n        # For now, return dummy data\n        return [{\'name\': \'object\', \'x\': 1.0, \'y\': 2.0, \'z\': 0.5}]\n        \n    def depth_callback(self, msg):\n        """\n        Process depth information for 3D localization\n        """\n        depth_image = self.bridge.imgmsg_to_cv2(msg, "32FC1")\n        \n        # Process depth information\n        # This would integrate with Isaac\'s depth processing tools\n        pass\n        \n    def odometry_callback(self, msg):\n        """\n        Process visual-inertial odometry from Isaac\n        """\n        # Update robot\'s position based on Isaac SLAM\n        # This would provide accurate localization for navigation\n        pass\n'})}),"\n",(0,i.jsx)(e.h3,{id:"object-identification-with-computer-vision",children:"Object Identification with Computer Vision"}),"\n",(0,i.jsx)(e.p,{children:"The object identification system combines traditional computer vision with deep learning:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import cv2\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image as PILImage\n\nclass ObjectIdentifier:\n    def __init__(self):\n        # Initialize pre-trained model (e.g., YOLO, SSD, etc.)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Load pre-trained detection model\n        # For this example, we'll use TorchVision's detection models\n        self.model = torch.hub.load(\n            'ultralytics/yolov5', 'yolov5s', pretrained=True\n        ).to(self.device)\n        \n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n        \n        # Define object categories of interest\n        self.target_objects = [\n            'person', 'chair', 'bottle', 'cup', 'book', \n            'cell phone', 'couch', 'dining table'\n        ]\n        \n    def identify_objects(self, image):\n        \"\"\"\n        Identify objects in the given image\n        \"\"\"\n        # Convert OpenCV image (BGR) to RGB\n        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Convert to PIL for model\n        pil_image = PILImage.fromarray(rgb_image)\n        \n        # Perform inference\n        results = self.model(pil_image)\n        \n        # Parse results\n        detections = []\n        for *xyxy, conf, cls in results.xyxy[0].tolist():\n            class_name = self.model.names[int(cls)]\n            \n            if class_name in self.target_objects and conf > 0.5:\n                detection = {\n                    'class': class_name,\n                    'confidence': conf,\n                    'bbox': [int(x) for x in xyxy],  # x1, y1, x2, y2\n                    'center': ((xyxy[0] + xyxy[2]) / 2, (xyxy[1] + xyxy[3]) / 2)\n                }\n                detections.append(detection)\n                \n        return detections\n        \n    def get_object_3d_position(self, bbox, depth_image):\n        \"\"\"\n        Estimate 3D position of object using bounding box and depth\n        \"\"\"\n        x1, y1, x2, y2 = bbox\n        \n        # Calculate center of bounding box\n        center_x, center_y = int((x1 + x2) / 2), int((y1 + y2) / 2)\n        \n        # Get depth at center point\n        depth = depth_image[center_y, center_x]\n        \n        if depth == 0 or np.isnan(depth):\n            # If depth is invalid, use average depth in bounding box\n            bbox_depth = depth_image[y1:y2, x1:x2]\n            valid_depths = bbox_depth[bbox_depth > 0]\n            if valid_depths.size > 0:\n                depth = np.mean(valid_depths)\n            else:\n                depth = 1.0  # Default if no valid depth found\n                \n        # Convert to 3D coordinates (simplified)\n        # In real implementation, use camera intrinsics\n        fx, fy = 554, 554  # Approximate focal lengths\n        cx, cy = 320, 240  # Approximate optical centers\n        \n        x = (center_x - cx) * depth / fx\n        y = (center_y - cy) * depth / fy\n        z = depth\n        \n        return np.array([x, y, z])\n"})}),"\n",(0,i.jsx)(e.h2,{id:"deployment-blueprint",children:"Deployment Blueprint"}),"\n",(0,i.jsx)(e.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,i.jsx)(e.p,{children:"For deploying an autonomous humanoid system, consider the following hardware requirements:"}),"\n",(0,i.jsx)(e.h4,{id:"computing-platform",children:"Computing Platform"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"GPU"}),": NVIDIA Jetson Orin AGX (for edge AI) or RTX 4080+ (for development)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"CPU"}),": Multi-core processor (8+ cores recommended)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Memory"}),": 32GB+ RAM"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Storage"}),": Fast SSD (1TB+ NVMe recommended)"]}),"\n"]}),"\n",(0,i.jsx)(e.h4,{id:"sensors",children:"Sensors"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cameras"}),": RGB-D camera (e.g., Intel Realsense D435i)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"IMU"}),": 9-axis IMU for balance and orientation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"LiDAR"}),": 2D or 3D LiDAR for navigation (optional but recommended)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Microphones"}),": Array of microphones for speech recognition"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Force/Torque Sensors"}),": For manipulation feedback"]}),"\n"]}),"\n",(0,i.jsx)(e.h4,{id:"actuators",children:"Actuators"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Motors"}),": High-torque servo motors for joints"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Controllers"}),": Real-time motor controllers"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Power"}),": High-capacity battery with power management"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"software-stack",children:"Software Stack"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Applications             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        Voice Interface              \u2502\n\u2502        Task Planning                \u2502\n\u2502        Navigation                   \u2502\n\u2502        Manipulation                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502             ROS 2                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        Isaac ROS Packages           \u2502\n\u2502        Perception Pipelines         \u2502\n\u2502        Navigation Stack             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         OS (Ubuntu 22.04)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(e.h3,{id:"deployment-configuration",children:"Deployment Configuration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-yaml",children:'# config/autonomous_humanoid.yaml\nhumanoid_config:\n  # Robot dimensions and physical properties\n  robot:\n    base_frame: "base_link"\n    odom_frame: "odom"\n    map_frame: "map"\n    height: 1.7  # meters\n    width: 0.5\n    mass: 60.0   # kg\n  \n  # Navigation parameters\n  navigation:\n    planner_frequency: 5.0\n    controller_frequency: 20.0\n    max_vel_x: 0.5\n    min_vel_x: 0.1\n    max_vel_theta: 1.0\n    min_in_place_vel_theta: 0.4\n    \n  # Perception parameters\n  perception:\n    camera_topic: "/camera/color/image_raw"\n    depth_topic: "/camera/depth/image_rect_raw"\n    detection_threshold: 0.5\n    tracking_timeout: 5.0  # seconds\n    \n  # Manipulation parameters\n  manipulation:\n    arm_joints: ["shoulder_pan", "shoulder_lift", "elbow_flex", "wrist_flex", "wrist_roll"]\n    grasp_tolerance: 0.05  # meters\n    max_grasp_force: 50.0  # Newtons\n    \n  # Voice interface\n  voice:\n    audio_topic: "/audio_input"\n    speech_model: "whisper-base"\n    wake_word: "humanoid"\n    timeout: 10.0  # seconds of silence before timeout\n'})}),"\n",(0,i.jsx)(e.h3,{id:"launch-file-configuration",children:"Launch File Configuration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# launch/autonomous_humanoid.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    \n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation clock if true'),\n        \n        # Isaac Visual SLAM\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='visual_slam_node',\n            name='visual_slam',\n            parameters=[{\n                'use_sim_time': use_sim_time,\n                'enable_rectification': True,\n                'input_width': 640,\n                'input_height': 480\n            }],\n            remappings=[('/visual_slam/image', '/camera/color/image_raw')],\n            output='screen'\n        ),\n        \n        # Object detection\n        Node(\n            package='vision_detections',\n            executable='object_detector',\n            name='object_detector',\n            parameters=[{\n                'use_sim_time': use_sim_time,\n                'detection_model': 'yolov5s'\n            }],\n            output='screen'\n        ),\n        \n        # Voice interface\n        Node(\n            package='voice_interface',\n            executable='voice_command_processor',\n            name='voice_command_processor',\n            parameters=[{\n                'use_sim_time': use_sim_time,\n                'audio_topic': '/audio_input',\n                'openai_api_key': 'YOUR_API_KEY'\n            }],\n            output='screen'\n        ),\n        \n        # Main autonomous humanoid node\n        Node(\n            package='autonomous_humanoid',\n            executable='autonomous_humanoid_node',\n            name='autonomous_humanoid',\n            parameters=[{\n                'use_sim_time': use_sim_time,\n                'config_file': 'config/autonomous_humanoid.yaml'\n            }],\n            output='screen'\n        ),\n    ])\n"})}),"\n",(0,i.jsx)(e.h3,{id:"deployment-steps",children:"Deployment Steps"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Hardware Setup"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Assemble humanoid robot platform"}),"\n",(0,i.jsx)(e.li,{children:"Connect sensors and actuators"}),"\n",(0,i.jsx)(e.li,{children:"Install computing platform with necessary interfaces"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Software Installation"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Install Ubuntu 22.04 LTS"}),"\n",(0,i.jsx)(e.li,{children:"Install ROS 2 Humble Hawksbill"}),"\n",(0,i.jsx)(e.li,{children:"Install NVIDIA Isaac ROS packages"}),"\n",(0,i.jsx)(e.li,{children:"Install required dependencies"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Configuration"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Calibrate sensors"}),"\n",(0,i.jsx)(e.li,{children:"Configure robot URDF"}),"\n",(0,i.jsx)(e.li,{children:"Set up network and communication"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Testing"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Individual component testing"}),"\n",(0,i.jsx)(e.li,{children:"Integration testing"}),"\n",(0,i.jsx)(e.li,{children:"Safety testing"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Deployment"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Final integration"}),"\n",(0,i.jsx)(e.li,{children:"Performance optimization"}),"\n",(0,i.jsx)(e.li,{children:"Continuous monitoring setup"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Emergency Stop"}),": Implement physical and software emergency stop mechanisms"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Collision Avoidance"}),": Continuous monitoring and avoidance of obstacles"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Force Limiting"}),": Limit forces during manipulation to prevent damage"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Operational Boundaries"}),": Define safe operational areas"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human-Robot Interaction"}),": Ensure safe interaction protocols"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"The autonomous humanoid system represents the culmination of all the technologies discussed in this book - from basic robotics principles to advanced AI integration. The system demonstrates how Physical AI enables robots to understand, navigate, and interact with the real world in meaningful ways."}),"\n",(0,i.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(e.p,{children:"This capstone chapter has demonstrated how to integrate all the concepts from this book into a functioning autonomous humanoid system. We've shown how to combine voice processing, task planning, navigation, perception, manipulation, and locomotion into a coherent system that can understand natural language commands and execute them through coordinated physical actions."}),"\n",(0,i.jsx)(e.p,{children:"The blueprint provided offers a roadmap for deploying such systems in real-world applications, though it should be customized based on specific requirements and constraints. The field of Physical AI continues to evolve rapidly, and we can expect even more capable and sophisticated autonomous humanoid systems in the future."}),"\n",(0,i.jsx)(e.p,{children:"The foundation provided in this book should enable practitioners to build upon these concepts and push the boundaries of what's possible in Physical AI and humanoid robotics."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);